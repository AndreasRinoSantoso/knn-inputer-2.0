{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-25T09:56:30.659556Z","iopub.execute_input":"2024-08-25T09:56:30.660708Z","iopub.status.idle":"2024-08-25T09:56:31.146238Z","shell.execute_reply.started":"2024-08-25T09:56:30.660655Z","shell.execute_reply":"2024-08-25T09:56:31.144929Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# dataset","metadata":{}},{"cell_type":"code","source":"data_campur = {\n    'Height': [175, 160, np.nan, 180, 165, 170, np.nan, 155, 180, 175],  # Continuous\n    'Weight': [70, np.nan, 65, 80, np.nan, 75, 85, 90, np.nan, 68],      # Continuous\n    'ShoeSize': [42, 38, 40, np.nan, 41, 39, 44, 37, 42, 43],            # Continuous\n    'Gender': ['Male', 'Female', 'Female', 'Male', np.nan, 'Female', 'Male', 'Female', 'Male', 'Male'],  # Categorical\n    'City': ['New York', 'Los Angeles', np.nan, 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', np.nan, 'San Antonio', 'San Diego'],  # Categorical\n    'Age': [25, 30, 22, 40, 35, np.nan, 29, 31, 28, 33],                 # Continuous\n    'MaritalStatus': ['Single', 'Married', 'Single', np.nan, 'Married', 'Single', 'Single', 'Married', np.nan, 'Single'],  # Categorical\n    'Salary': [50000, 55000, np.nan, 60000, 58000, 62000, 65000, np.nan, 57000, 59000]  # Continuous\n}\n\n# Membuat DataFrame dengan kolom campuran (continuous dan categorical)\ngabung1 = pd.DataFrame(data_campur)\n\n# Menampilkan DataFrame dengan nilai NaN\nprint(gabung1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_with_number = [col for col in gabung1 if gabung1[col].dtype in ['int64', 'float64']]\ncolumns_with_word = [col for col in gabung1 if gabung1[col].dtype in ['bool', 'object']]\ngabung1[columns_with_word]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fungsi knn data gabung1 yg nilainya nan namun hanya menghitung jarak antar kolom categorikal saja","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\ndef hamming_distance(gabung1, columns_with_word):\n    # Cari baris yang memiliki nilai NaN di gabung1\n    original_row_indices = gabung1.index[gabung1.isnull().any(axis=1)].tolist()\n    \n    # Buat matrix jarak Hamming hanya untuk baris yang memiliki nilai NaN di gabung1 dan kolom columns_with_word\n    distance_matrix = np.zeros((len(original_row_indices), len(gabung1)))\n    \n    # Vectorized calculation of Hamming distance\n    for i, nan_row in enumerate(tqdm(original_row_indices, desc=\"Calculating Hamming distance\")):\n        distance_matrix[i] = (gabung1[columns_with_word].ne(gabung1.loc[nan_row, columns_with_word], axis=1).sum(axis=1))\n    \n    # Set diagonal elements to 0\n    distance_matrix[np.arange(len(original_row_indices)), original_row_indices] = 0\n    \n    # Convert distance matrix to dataframe\n    df_result = pd.DataFrame(distance_matrix, index=original_row_indices, columns=gabung1.index)\n    \n    return df_result\n\ndistance_categorikal = hamming_distance(gabung1, columns_with_word)\nprint(distance_categorikal)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fungsi knn data gabung1 yg nilainya nan namun hanya menghitung jarak antar kolom continousnya saja","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(gabung1[columns_with_number])\n\n# Fungsi untuk menghitung Euclidean Distance dengan NaN dianggap sebagai 1\ndef euclidean_distance(x1, x2):\n    diff = np.abs(x1 - x2)    \n    diff[np.isnan(diff)] = 1    \n    diff[np.isnan(x1) & np.isnan(x2)] = 1  # Jika keduanya NaN, jaraknya dianggap 0    \n    distance = np.nansum(diff ** 2)    \n    return distance\n\ndef calculate_distances(X, nan_rows, original_df):\n    num_rows = X.shape[0]\n    distances = np.zeros((len(nan_rows), num_rows))\n    \n    for i, nan_row in enumerate(tqdm(nan_rows, desc=\"Calculating distances for NaN rows\")):\n        for j in range(num_rows):\n            distances[i, j] = euclidean_distance(X[nan_row], X[j])\n    \n    return distances\n\n# Cari baris yang memiliki nilai NaN di gabung1\nnan_rows = gabung1.index[gabung1.isnull().any(axis=1)].tolist()\n\n# Menghitung jarak antar baris setelah di-scaler hanya untuk baris dengan NaN di gabung1 dan kolom columns_with_number\ndistances = calculate_distances(X_scaled, nan_rows, gabung1)\n\n# Konversi hasil jarak ke DataFrame\ndistances_continous = pd.DataFrame(distances, index=nan_rows, columns=gabung1.index)\n\n# Set value to 0 when row index matches column index\nfor i in distances_continous.index:\n    distances_continous.loc[i, i] = 0\n\n# Tampilkan hasil jarak antar baris dalam DataFrame\nprint(\"\\nJarak Euclidean antar baris continous setelah di-scaler (dengan aturan NaN):\")\nprint(distances_continous)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# gabungin dan mengisi row yg hilang dengan 0","metadata":{}},{"cell_type":"code","source":"distance_matrix=distances_continous+distance_categorikal\nprint(distance_matrix)\ndistance_matrix =distance_matrix.map(np.sqrt)\nprint('\\n hasil pengakaran')\nprint(distance_matrix)\ndistance_matrix = distance_matrix.reindex(gabung1.index, fill_value=0)\nprint('isi nilai dengan 0')\nprint(distance_matrix)\ndistance_matrix=distance_matrix.to_numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# input knn","metadata":{}},{"cell_type":"code","source":"df =gabung1\n\ndef knn_impute(df, distance_matrix, k=2):\n    imputed_data = df.copy()\n    col_name = df.columns[0]\n    for i in range(len(df)):\n        if np.isnan(df.iloc[i][col_name]):\n            print(f\"Imputing missing value at row {i}, column {col_name}...\")\n            distances = distance_matrix[i]\n            neighbors = np.argsort(distances)[1:]  # exclude self\n            print(f\"Neighbors: {neighbors}\")\n            print(f\"Neighbor values:\")\n            neighbor_values = []\n            for neighbor in neighbors:\n                value = df.iloc[neighbor, 0]\n                print(f\"  Neighbor {neighbor}: {value}\")\n                if not np.isnan(value):\n                    neighbor_values.append((neighbor, value))\n                if len(neighbor_values) == k:\n                    break\n            print(f\"Using neighbors: {[neighbor for neighbor, _ in neighbor_values]}\")\n            if neighbor_values:\n                imputed_value = np.mean([value for _, value in neighbor_values])\n                print(f\"Imputed value: {imputed_value}\")\n                imputed_data.iloc[i, 0] = imputed_value\n            else:\n                print(\"No non-missing neighbors, recursively calling the function...\")\n                imputed_data.iloc[neighbors, :] = knn_impute(imputed_data.iloc[neighbors, :], distance_matrix, k)\n                imputed_data.iloc[i, 0] = np.nanmean(imputed_data.iloc[neighbors, 0])\n                print(f\"Imputed value: {imputed_data.iloc[i, 0]}\")\n            print()\n    return imputed_data\n\ndef knn_impute_categorical(df, distance_matrix, k=3):\n    imputed_data = df.copy()\n    col_name = df.columns[0]\n    for i in range(len(df)):\n        if df.iloc[i][col_name] is np.nan:\n            print(f\"Imputing missing value at row {i}, column {col_name}...\")\n            \n            neighbors = np.argsort(distance_matrix[i])[:k+1][1:]  # exclude self\n            print(f\"Neighbors: {neighbors}\")\n            # Get categorical values from neighbors\n            neighbor_values = [df.iloc[neighbor, 0] for neighbor in neighbors]\n            print(\"Neighbor values:\")\n            for neighbor, value in zip(neighbors, neighbor_values):\n                print(f\"  Neighbor {neighbor}: {value}\")\n            # Remove NaN values from neighbor_values\n            non_nan_values = [(value, distance_matrix[i, neighbors[j]]) for j, value in enumerate(neighbor_values) if value is not np.nan]\n            if non_nan_values:\n                # Count the occurrences of each value\n                value_counts = {}\n                for value, _ in non_nan_values:\n                    value_counts[value] = value_counts.get(value, 0) + 1\n                # Find the mode\n                max_count = max(value_counts.values())\n                modes = [value for value, count in value_counts.items() if count == max_count]\n                if len(modes) == 1:\n                    # If there's only one mode, use it\n                    imputed_value = modes[0]\n                else:\n                    # If there are multiple modes, use the one with the closest neighbor\n                    closest_mode = min([(value, distance) for value, distance in non_nan_values if value in modes], key=lambda x: x[1])\n                    imputed_value = closest_mode[0]\n            else:\n                # If all neighbors have NaN values, impute with the most frequent value in the column\n                imputed_value = df[col_name].value_counts().index[0]\n            print(f\"Using neighbors: {neighbors}\")\n            print(f\"Imputed value: {imputed_value}\")\n            print()\n            imputed_data.iloc[i, 0] = imputed_value\n    return imputed_data\nimputed_df = df.copy()\nfor col in df.columns:\n    if df[col].dtype.kind in ['i', 'f']:  # continuous column\n        imputed_df[col] = knn_impute(imputed_df[[col]], distance_matrix, k=4)[col]\n    else:  # categorical column\n        imputed_df[col] = knn_impute_categorical(imputed_df[[col]], distance_matrix, k=4)[col]\nprint(\"Imputed dataframe:\")\nprint(imputed_df)","metadata":{},"execution_count":null,"outputs":[]}]}